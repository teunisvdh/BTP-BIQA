{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraper DPchallenge.com\n",
    "\n",
    "### Goal\n",
    "Since the AVA dataset constists of ~250.000 images it is too large to train the decision tree developed. Additionally the dataset includes heavy edited/artistic images or image subjects not interesting for the purpose of the decision tree: detecting quality of images created during social events. Therefore I decided to create a new dataset containing only images with a relevant subject: sports, concert and formal events. \n",
    "\n",
    "### Script\n",
    "For each of the three themes I composed a list of search terms that could be used to search for relevant images on the website of https://dpchallenge.com/. For each search request it collects the pages where the images can be found and subsequently collects the image links and scores. \n",
    "This resulted in a final dataframe of image id's, image links, image scores and image themes. \n",
    "\n",
    "### Additional Remarks\n",
    "Make sure that web scraping can cause overloads on the servers of dpchallenge. Please include some delays in between requests.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests_html import HTMLSession\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://jonathansoma.com/lede/foundations-2017/classes/adv-scraping/advanced-scraping-form-submission/ \n",
    "def get_pages_search(term):\n",
    "    \n",
    "    data = {\n",
    "    'MATCH': 'bool',\n",
    "    'search_title': '1',\n",
    "    'search_keywords': '1',\n",
    "    'search_description': '1',\n",
    "    'filter_challenge': '1',\n",
    "    'order': 'r',\n",
    "    'q': term,\n",
    "    }\n",
    "    \n",
    "    number_pages = []\n",
    "    all_pages = []\n",
    "    url = 'https://www.dpchallenge.com/photo_search.php'\n",
    "    response = requests.post(url, data = data)\n",
    "    doc = bs(response.text, 'html.parser')\n",
    "    hyperlinks = doc.find_all(\"a\", {\"class\": \"u\"}, href=True)\n",
    "    for page in hyperlinks:\n",
    "        str_page = str(page)\n",
    "        if \"/photo_search.php?IMAGE_SEARCH_ID\" in str_page and \"next.gif\" not in str_page:\n",
    "            base_url = re.split('href=\"|amp;page=', str_page)[1]\n",
    "            number = re.split('page=|\"><img border=\"0\"', str_page)\n",
    "            number_pages.append(int(number[1]))\n",
    "\n",
    "    max_page = np.max(np.array(number_pages))\n",
    "    for i in range (1, max_page + 1):\n",
    "        hyperlink = 'https://www.dpchallenge.com' + base_url +\"page=\" + str(i)\n",
    "        all_pages.append(hyperlink)  \n",
    "    return(all_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_links(pages):\n",
    "    image_links = []\n",
    "    for link in pages:\n",
    "        page = requests.get(link)\n",
    "        soup = bs(page.content, 'html.parser')\n",
    "        image_table = soup.find('table', {'cellpadding': '3', 'width': '100%', 'cellspacing': '0'})\n",
    "        image_columns = image_table.find_all('td')\n",
    "        for column in image_columns:\n",
    "            string_column = re.split('href=\"|\">', str(column))[2]\n",
    "            hyperlink = 'https://www.dpchallenge.com' + string_column\n",
    "            image_links.append(hyperlink)\n",
    "    return(image_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_by_link(link, theme):\n",
    "    try:\n",
    "        image_id = link.split('IMAGE_ID=')[1]\n",
    "        proxy = {'http': '181.209.82.154:23500', 'https':'181.209.82.154:23500'}\n",
    "        page = requests.get(link, proxies = proxy)\n",
    "        soup = bs(page.content, 'html.parser')\n",
    "        images = soup.find('td', {'id': 'img_container'})\n",
    "        found = str(images).split(\"src=\")[2].split('\"')[1]\n",
    "        img_link = str(\"https:\" + found)\n",
    "        print(img_link)\n",
    "        statistics = soup.find_all('table', {'width': '750'})[1]\n",
    "        img_score = str(statistics).split('Avg (all users):</b> ')[1]\n",
    "        img_score = img_score.split('<br/>')[0]\n",
    "        return(image_id, img_link, img_score, theme)\n",
    "    except:\n",
    "        return(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_theme(theme):\n",
    "    pages = get_pages_search(theme)\n",
    "    image_link_list = image_links(pages)\n",
    "    for image_link in image_link_list:\n",
    "        image = image_by_link(image_link, theme)\n",
    "        if image != 0:\n",
    "            data_dpchallenge.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "themes = ['sports', 'concert', 'formal']\n",
    "sports = ['soccer', 'football', 'basketball', 'cycling', 'racing', 'stadium']\n",
    "concert = ['festival', 'concert', 'music', 'disco', 'dancing', 'nightlife'] \n",
    "formal = ['exhibition', 'expo', 'wedding', 'conversation'] \n",
    "\n",
    "dict_themes = {}\n",
    "for theme in themes:\n",
    "    dict_themes[theme] = globals()[theme]\n",
    "\n",
    "for key in dict_themes:\n",
    "    terms = dict_themes[key]\n",
    "    for word in terms:\n",
    "        get_images_theme(word)\n",
    "\n",
    "data_df = pd.DataFrame(data_dpchallenge, columns = ['id', 'link', 'score', 'theme'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
